{
  "created_at": "2012-06-30T03:18:45Z",
  "id": 386,
  "resolved": true,
  "status_dev": "green",
  "status_prod": "green",
  "title": "Widespread Application Outage",
  "upcoming": false,
  "updated_at": "2012-07-07T16:55:45Z",
  "href": "https://status.cf.com/api/v3/issues/386",
  "updates": [
    {
      "contents": "On Friday June 29th, Cloud Foundry customers experienced a disruption in service which affected running applications and Cloud Foundry Postgres databases. We deeply regret the effects of this incident on our customers, and accept full responsibility for the downtime they experienced. We would like to share some additional technical detail about what happened, the steps we are taking in response, and actions that customers can take to protect themselves.\r\n\r\n## Scope of impact\r\n\r\n### For all applications\r\n\r\n* Cloud Foundry API access was limited during recovery operations\r\n * The API was in maintenance mode or read-only mode for 3 hours 41 minutes\r\n * Application deployments were disallowed for an additional 1 hour 30 minutes\r\n* Customers experienced degraded performance due to lost capacity\r\n\r\n### For applications using the Cedar stack\r\n\r\n* About 30% of applications lost one or more dynos for 2 hours 27 minutes\r\n* Dynos were gradually restored over the following 2 hours 50 minutes\r\n\r\n### For applications using the Bamboo stack\r\n* Total outage for all Bamboo applications for 3 hours 50 minutes\r\n* Intermittent failures and degraded performance for an additional 51 minutes\r\n* Degraded performance due to lack of HTTP caching for an additional 9 hours 30 minutes\r\n\r\n### For Cloud Foundry Postgres databases\r\n\r\n* 20% of production databases experienced up to 7 hours of downtime\r\n* A further 8% experienced an additional 10 hours of downtime (up to 17 hours total)\r\n* All production databases were successfully recovered from continuous backups\r\n* Some Beta and shared databases were offline for a further 6 hours (up to 23 hours total)\r\n* Approximately 0.02% of shared/development databases were restored from a daily backup where continuous backups were not available\r\n\r\n### Third party services\r\n\r\nMany customers who relied on third party services and addons built on EBS were also significantly impacted.\r\n\r\n## What happened\r\n\r\nThe disruption was precipitated by an Amazon Web Services outage which affected the US East region, beginning at 8:04PM PDT. AWS has published a [summary](http://aws.amazon.com/message/67457/) of the incident and their response to it, which goes into more detail about the underlying causes of the events discussed here.\r\n\r\nThe direct impact of this outage on Cloud Foundry was twofold:\r\n\r\n### 1. Lost EC2 instances\r\n\r\nApproximately 30% of our EC2 instances, which were responsible for running applications, databases and supporting infrastructure (including some components specific to the Bamboo stack), went offline.\r\n\r\nWe intend to insulate our customers from the loss of any EC2 instances, and had prepared for such a scenario by developing fault tolerant infrastructure, automated failover systems, and emergency response procedures to enable us to quickly restore service. Unfortunately, our recovery efforts were severely hampered by subsequent infrastructure failures, and the result was an extended outage. Complications included:\r\n\r\n* The management API for the AWS US East region became unavailable\r\n * In order to restore sufficient capacity quickly, we needed to bring additional instances online in the same region. Without the API, we couldn’t start any new instances.\r\n * We regularly rely on this API in order to collect information about the state of our infrastructure, and to diagnose problems. This lack of visibility slowed the recovery process.\r\n* Elastic Load Balancer instances and Elastic IP addresses failed to respond promptly to configuration changes\r\n * We use ELBs and EIPs to redirect traffic away from failed instances to redundant and secondary systems, so when these mechanisms responded slowly or malfunctioned, traffic continued to be routed to systems which were down.\r\n\r\nAs a matter of course, we disabled and limited access to the Cloud Foundry API while the platform was stabilized.\r\n\r\n### 2. Lost Elastic Block Storage volumes\r\n\r\nA large number of EBS volumes, which stored data for Cloud Foundry Postgres services, went offline and their data was potentially corrupted.  As a result, customer databases remained down until they could be recovered, unless a follower database was activated (see below for more about followers). As a precaution against this type of incident, we continuously archive customer data for recovery purposes, and many databases needed to be restored from these archives.\r\n\r\nIn past EBS incidents, it was very rare for volumes to be damaged even if they sustained downtime, so they were very fast to recover. We were therefore not fully prepared for a recovery effort of this magnitude, and it took several hours to automatically restore the affected databases.\r\n\r\nIn the end, all production databases were successfully recovered and normal operations were restored.\r\n\r\n## What we are doing\r\n\r\nWe are planning a comprehensive remediation effort to address the factors which led to the downtime.\r\n\r\n### Infrastructure failures\r\n\r\nWe will continue to work to increase our resilience to infrastructure outages and to improve our ability to recover from a wide range of failure modes, including the loss of instances and EBS volumes. Our #1 goal is to provide a trustworthy platform regardless of the underlying infrastructure. \r\n\r\n### Database availability\r\n\r\nAs a result of this outage, we have produced new tools which enable us to more expediently relocate database services from a failed availability zone. We will further invest in these tools and make them a part of our daily operations, to ensure that identifying and relocating affected resources goes smoothly in the future.\r\n\r\n### Failover mechanisms\r\n\r\nSome of our failover mechanisms did not perform adequately during this outage, because of their reliance on certain AWS services, including the US East region API, Elastic Load Balancers, and Elastic IPs, which were affected by the outage. We will investigate ways to make these mechanisms more resilient to such failures, and consider alternative solutions where they are available.\r\n\r\n### Bamboo routing\r\n\r\nWe do not plan to invest further in improving robustness for applications using the older Bamboo stack. Instead, we will focus on improving the current Cedar stack, and we encourage customers to migrate to Cedar, where we are investing all of our application availability efforts.\r\n\r\n## What you can do\r\n\r\nWhile we strive to make the Cloud Foundry platform as a whole as resilient as possible, the design and configuration of applications can significantly improve their resiliency. Here are some recommended steps you can take to improve the availability of your app.\r\n\r\n### [Improve Cloud Foundry Postgres availability with a Follower](https://devcenter.cf.com/articles/improving-cf-postgres-availability-with-followers)\r\n\r\nCloud Foundry Postgres offers a simple way to maintain a continuously updated replica of your database, called a [follower](https://devcenter.cf.com/articles/cf-postgresql#follow_beta). Followers can answer read-only queries, and if your primary database fails, you can easily “promote” the follower to be a new, writable primary by running two commands.\r\n\r\n### [Migrate your applications to the Cedar stack](https://devcenter.cf.com/articles/cedar-migration)\r\n\r\nWe have made many infrastructure improvements in the past year which only benefit customers using the current Cedar stack. Customers using Cedar experienced, on average, only 1/10th the downtime compared to those using Bamboo.\r\n\r\n### [Run multiple dynos in production applications](https://devcenter.cf.com/articles/dynos#redundancy)\r\n\r\nApplications with multiple running dynos will be more resilient against failure. If some dynos are lost, the application can continue to process requests while the missing dynos are replaced. Typically, lost dynos are replaced immediately, but in the case of a catastrophic failure like this one, it can take some time.\r\n\r\n### Summary\r\n\r\nWe take our availability very seriously. We are taking steps both to address the specific issues raised by this outage, and to continue to improve the overall resiliency of our platform to infrastructure instability. The improvements we have made over the past year materially reduced the downtime experienced during this incident, but we recognize we have more work to do. Our entire team is focused on making Cloud Foundry the most trustworthy platform available for our customers.",
      "created_at": "2012-07-11T20:59:12Z",
      "id": 1220,
      "incident_id": 386,
      "status_dev": "green",
      "status_prod": "green",
      "title": "Widespread Application Outage",
      "update_type": "follow-up",
      "updated_at": "2012-07-12T00:48:25Z"
    },
    {
      "contents": "App operations are restored at this time. We are monitoring the situation, and tracking additional issues that arrise in separate status incidents.",
      "created_at": "2012-06-30T16:55:44Z",
      "id": 1170,
      "incident_id": 386,
      "status_dev": "green",
      "status_prod": "green",
      "title": "Widespread Application Outage",
      "update_type": "resolved",
      "updated_at": "2012-06-30T17:02:58Z"
    },
    {
      "contents": "Dedicated production databases should be operational. We're continuing recovery of shared development databases and Crane and Kappa beta databases. Varnish remains temporarily disabled on the Bamboo HTTP stack.",
      "created_at": "2012-06-30T14:10:01Z",
      "id": 1167,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T14:10:51Z"
    },
    {
      "contents": "HTTP routing is improving on Bamboo although the HTTP caching layer has been temporarily disabled on this stack.\r\n\r\nDatabase recovery efforts are ongoing.",
      "created_at": "2012-06-30T12:21:55Z",
      "id": 1166,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T12:22:29Z"
    },
    {
      "contents": "We are experiencing increased error rates on the Bamboo HTTP stack.",
      "created_at": "2012-06-30T12:01:35Z",
      "id": 1165,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T12:01:38Z"
    },
    {
      "contents": "HTTP error levels on the Bamboo stack have returned to normal.\r\n\r\nSome applications continue to be affected by unavailable database services. We are still working on restoring these.",
      "created_at": "2012-06-30T10:19:35Z",
      "id": 1164,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T10:19:37Z"
    },
    {
      "contents": "There is an increase in HTTP errors on applications that run on the Bamboo stack. We are working on resolving this.",
      "created_at": "2012-06-30T09:44:56Z",
      "id": 1163,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T09:57:38Z"
    },
    {
      "contents": "HTTP error rates have returned to normal levels. Deployment via git push have been re-enabled, but we are seeing elevated error rates for this service. We continue to work to resolve this issue.\r\n\r\nWhile the majority of applications are online and operating normally, a number of applications continue to be affected by unavailable database services. Additionally, some addon providers have also been affected by tonight's electrical storm. While these services are beyond our control, our addon providers are also working diligently to restore service.",
      "created_at": "2012-06-30T08:31:51Z",
      "id": 1162,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T08:31:53Z"
    },
    {
      "contents": "API operations have been restored, but git pushes remain disabled. We continue to see elevated error rates across our HTTP stack (such as H99s).",
      "created_at": "2012-06-30T08:11:14Z",
      "id": 1161,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T08:11:19Z"
    },
    {
      "contents": "We're seeing elevated HTTP error rates across the platform. Our engineers are currently investigating.",
      "created_at": "2012-06-30T07:41:21Z",
      "id": 1160,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T07:41:24Z"
    },
    {
      "contents": "We're seeing elevated error rates to our API which manifests as non-responsive API calls. For example, `cf` commands from the command-line may not return. Our engineers are working to address this issue.",
      "created_at": "2012-06-30T07:34:37Z",
      "id": 1159,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T07:34:43Z"
    },
    {
      "contents": "The majority of applications are online. Our infrastructure provider has informed us that due to the electrical storm which interrupted power, possible disk corruption may have occurred on affected databases. We are recovering all at-risk databases from our continuous-protection archives.",
      "created_at": "2012-06-30T07:17:18Z",
      "id": 1158,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T07:17:19Z"
    },
    {
      "contents": "The majority of applications are online. A number of applications that rely on unavailable \r\ndatabases are still offline. We are working to reduce error rates and restore database services. Full API access is now available.",
      "created_at": "2012-06-30T06:52:52Z",
      "id": 1157,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T06:52:53Z"
    },
    {
      "contents": "We continue to work to stabilize the platform and restore database services. The API is now available in read-only mode.",
      "created_at": "2012-06-30T06:35:27Z",
      "id": 1156,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T06:35:28Z"
    },
    {
      "contents": "We've restored availablility to many applications, but continue to see fluctuations in error rates. A number of applications have processes which are unavailable or are relying on database services which are still offline. We are working on stabilizing the platform and restoring database services.",
      "created_at": "2012-06-30T05:59:32Z",
      "id": 1155,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T05:59:34Z"
    },
    {
      "contents": "We've restored the majority of internal services and are seeing a reduction in error rates, but many applications and databases remain offline. We are continuing to work to restore processes and databases.",
      "created_at": "2012-06-30T05:35:10Z",
      "id": 1154,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T05:35:29Z"
    },
    {
      "contents": "Our engineers continue to work to restore affected services and bring new capacity online. Our data team is also working to restore unavailable databases.",
      "created_at": "2012-06-30T05:06:06Z",
      "id": 1153,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T05:06:24Z"
    },
    {
      "contents": "Our engineers are continuing to move services away from affected infrastructure. We'll provide further updates as we progress.",
      "created_at": "2012-06-30T04:33:03Z",
      "id": 1152,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T04:55:13Z"
    },
    {
      "contents": "Our engineers continue to work to restore affected systems. Some production applications are unaffected but many applications are offline. API access is disabled while we restore service.",
      "created_at": "2012-06-30T04:05:15Z",
      "id": 1151,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T04:05:34Z"
    },
    {
      "contents": "We have lost connectivity to some of our infrastructure. Our engineers are working to restore affected systems. We've disabled API access while we work through the issues.",
      "created_at": "2012-06-30T03:49:00Z",
      "id": 1150,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "update",
      "updated_at": "2012-06-30T03:49:17Z"
    },
    {
      "contents": "We're currently experiencing a widespread application outage. We've disabled API access while engineers work on resolving the issues.",
      "created_at": "2012-06-30T03:25:25Z",
      "id": 1149,
      "incident_id": 386,
      "status_dev": "red",
      "status_prod": "red",
      "title": "Widespread Application Outage",
      "update_type": "issue",
      "updated_at": "2012-06-30T03:25:43Z"
    },
    {
      "contents": "Our automated systems have detected potential platform errors.  We are investigating.\r\n",
      "created_at": "2012-06-30T03:18:45Z",
      "id": 1148,
      "incident_id": 386,
      "status_dev": "yellow",
      "status_prod": "yellow",
      "title": "Potential Platform Issues",
      "update_type": "investigating",
      "updated_at": "2012-06-30T03:18:45Z"
    }
  ]
}
